version: '3.8'

# =============================================================================
# PRODUCTION ENVIRONMENT CONFIGURATION
# Usage: docker-compose -f docker-compose.base.yml -f docker-compose.prod.yml up
# =============================================================================

services:
  # =============================================================================
  # DATABASE SERVICES - PRODUCTION CONFIGURATION
  # =============================================================================
  postgres:
    container_name: pwc-postgres-prod
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C"
    ports:
      - "${POSTGRES_PORT}:5432"
    volumes:
      - postgres_prod_data:/var/lib/postgresql/data
      - ./docker/init-db-prod.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
      - ./backups/postgres:/backups:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  # =============================================================================
  # MESSAGE QUEUE & STREAMING - PRODUCTION CONFIGURATION
  # =============================================================================
  rabbitmq:
    container_name: pwc-rabbitmq-prod
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD}
      RABBITMQ_ERLANG_COOKIE: ${RABBITMQ_ERLANG_COOKIE}
      RABBITMQ_VM_MEMORY_HIGH_WATERMARK: 0.6
    ports:
      - "${RABBITMQ_PORT}:5672"
      - "${RABBITMQ_MANAGEMENT_PORT}:15672"
    volumes:
      - rabbitmq_prod_data:/var/lib/rabbitmq
      - ./docker/rabbitmq/prod:/etc/rabbitmq:ro
      - ./backups/rabbitmq:/backups:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  zookeeper:
    container_name: pwc-zookeeper-prod
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_MAX_CLIENT_CNXNS: 60
    ports:
      - "${ZOOKEEPER_PORT}:2181"
    volumes:
      - zookeeper_prod_data:/var/lib/zookeeper/data
      - zookeeper_prod_logs:/var/lib/zookeeper/log
      - ./backups/zookeeper:/backups:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  kafka:
    container_name: pwc-kafka-prod
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://${KAFKA_EXTERNAL_HOST:-localhost}:${KAFKA_PORT}
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 3000
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: ${KAFKA_EXTERNAL_HOST:-localhost}
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'  # Disable auto-creation in prod
      KAFKA_DELETE_TOPIC_ENABLE: 'false'        # Disable topic deletion
      KAFKA_LOG_RETENTION_HOURS: ${KAFKA_LOG_RETENTION_HOURS:-168}
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_NUM_PARTITIONS: ${KAFKA_NUM_PARTITIONS:-6}
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
    ports:
      - "${KAFKA_PORT}:9092"
      - "${KAFKA_JMX_PORT}:9101"
    volumes:
      - kafka_prod_data:/var/lib/kafka/data
      - ./backups/kafka:/backups:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  # Additional Kafka brokers for production cluster
  kafka-2:
    image: confluentinc/cp-kafka:7.4.0
    container_name: pwc-kafka-2-prod
    hostname: kafka-2
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_DELETE_TOPIC_ENABLE: 'false'
      KAFKA_LOG_RETENTION_HOURS: ${KAFKA_LOG_RETENTION_HOURS:-168}
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
    volumes:
      - kafka_2_prod_data:/var/lib/kafka/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    networks:
      - backend-network
    profiles: ["kafka-cluster"]

  kafka-3:
    image: confluentinc/cp-kafka:7.4.0
    container_name: pwc-kafka-3-prod
    hostname: kafka-3
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_DELETE_TOPIC_ENABLE: 'false'
      KAFKA_LOG_RETENTION_HOURS: ${KAFKA_LOG_RETENTION_HOURS:-168}
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
    volumes:
      - kafka_3_prod_data:/var/lib/kafka/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    networks:
      - backend-network
    profiles: ["kafka-cluster"]

  # =============================================================================
  # SPARK CLUSTER - PRODUCTION CONFIGURATION
  # =============================================================================
  spark-master:
    container_name: pwc-spark-master-prod
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_ENCRYPTION_ENABLED=yes
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=yes
      - SPARK_SSL_ENABLED=${SPARK_SSL_ENABLED:-no}
      - SPARK_USER=spark
      - SPARK_MASTER_OPTS=-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/spark-recovery
    ports:
      - "${SPARK_MASTER_WEB_UI_PORT}:8080"
      - "${SPARK_MASTER_PORT}:7077"
    volumes:
      - ./data/prod:/app/data
      - spark_prod_recovery:/spark-recovery
      - spark_prod_warehouse:/opt/spark/spark-warehouse
      - ./backups/spark:/backups:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  # Multiple Spark workers for production
  spark-worker-1:
    image: bitnami/spark:3.5.3
    container_name: pwc-spark-worker-1-prod
    hostname: spark-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY:-4G}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES:-4}
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_ENCRYPTION_ENABLED=yes
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=yes
      - SPARK_USER=spark
    volumes:
      - ./data/prod:/app/data
      - spark_prod_warehouse:/opt/spark/spark-warehouse
    depends_on:
      spark-master:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 6G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    networks:
      - backend-network
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  spark-worker-2:
    image: bitnami/spark:3.5.3
    container_name: pwc-spark-worker-2-prod
    hostname: spark-worker-2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY:-4G}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES:-4}
      - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      - SPARK_RPC_ENCRYPTION_ENABLED=yes
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=yes
      - SPARK_USER=spark
    volumes:
      - ./data/prod:/app/data
      - spark_prod_warehouse:/opt/spark/spark-warehouse
    depends_on:
      spark-master:
        condition: service_healthy
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 6G
          cpus: '4.0'
        reservations:
          memory: 4G
          cpus: '2.0'
    networks:
      - backend-network
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

  # =============================================================================
  # VECTOR DATABASE - PRODUCTION CONFIGURATION
  # =============================================================================
  typesense:
    container_name: pwc-typesense-prod
    environment:
      TYPESENSE_API_KEY: ${TYPESENSE_API_KEY}
      TYPESENSE_ENABLE_CORS: false  # Disabled in production
    ports:
      - "${TYPESENSE_PORT}:8108"
    volumes:
      - typesense_prod_data:/data
      - ./backups/typesense:/backups:ro
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  # =============================================================================
  # MONITORING - PRODUCTION CONFIGURATION
  # =============================================================================
  datadog-agent:
    container_name: pwc-datadog-prod
    environment:
      DD_HOSTNAME: pwc-production
      DD_ENV: production
      DD_LOGS_CONFIG_CONTAINER_COLLECT_ALL: true
      DD_PROCESS_AGENT_ENABLED: true
      DD_NETWORK_MONITORING_ENABLED: true
      DD_SECURITY_COMPLIANCE_ENABLED: true
      DD_RUNTIME_SECURITY_ENABLED: true
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Production monitoring stack
  prometheus:
    image: prom/prometheus:latest
    container_name: pwc-prometheus-prod
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./docker/prometheus/prod.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_prod_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
    networks:
      - monitoring-network
      - backend-network
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  grafana:
    image: grafana/grafana:latest
    container_name: pwc-grafana-prod
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    volumes:
      - grafana_prod_data:/var/lib/grafana
      - ./docker/grafana/prod:/etc/grafana/provisioning:ro
      - ./ssl/grafana:/etc/ssl/certs:ro
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_USERS_ALLOW_ORG_CREATE=false
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_COOKIE_SAMESITE=strict
      - GF_SERVER_PROTOCOL=${GRAFANA_PROTOCOL:-http}
      - GF_SERVER_DOMAIN=${GRAFANA_DOMAIN:-localhost}
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
    networks:
      - monitoring-network
      - frontend-network
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  # =============================================================================
  # APPLICATION SERVICES - PRODUCTION CONFIGURATION
  # =============================================================================
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.production
      target: production-api
    container_name: pwc-api-prod
    environment:
      ENVIRONMENT: production
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      RABBITMQ_HOST: rabbitmq
      RABBITMQ_USER: ${RABBITMQ_USER}
      RABBITMQ_PASSWORD: ${RABBITMQ_PASSWORD}
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      TYPESENSE_HOST: typesense
      TYPESENSE_API_KEY: ${TYPESENSE_API_KEY}
      SPARK_MASTER_URL: spark://spark-master:7077
      SECRET_KEY: ${SECRET_KEY}
      BASIC_AUTH_PASSWORD: ${BASIC_AUTH_PASSWORD}
      # Production-specific settings
      LOG_LEVEL: INFO
      DEBUG: false
      WORKERS: ${API_WORKERS:-4}
      MAX_REQUESTS: ${API_MAX_REQUESTS:-1000}
      MAX_REQUESTS_JITTER: ${API_MAX_REQUESTS_JITTER:-100}
    volumes:
      - ./data/prod:/app/data:ro
      - ./logs/prod:/app/logs
    ports:
      - "${API_PORT}:8000"
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
      replicas: 2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
    security_opt:
      - no-new-privileges:true

  # =============================================================================
  # ETL SERVICES - PRODUCTION CONFIGURATION
  # =============================================================================
  etl-bronze:
    build:
      context: .
      dockerfile: docker/Dockerfile.production
      target: production-etl
    container_name: pwc-etl-bronze-prod
    environment:
      ENVIRONMENT: production
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      SPARK_MASTER_URL: spark://spark-master:7077
      PROCESSING_ENGINE: spark
      LOG_LEVEL: INFO
    volumes:
      - ./data/prod:/app/data:ro
      - ./logs/prod:/app/logs
    restart: "no"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  etl-silver:
    build:
      context: .
      dockerfile: docker/Dockerfile.production
      target: production-etl
    container_name: pwc-etl-silver-prod
    environment:
      ENVIRONMENT: production
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      SPARK_MASTER_URL: spark://spark-master:7077
      PROCESSING_ENGINE: spark
      LOG_LEVEL: INFO
    volumes:
      - ./data/prod:/app/data:ro
      - ./logs/prod:/app/logs
    restart: "no"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  etl-gold:
    build:
      context: .
      dockerfile: docker/Dockerfile.production
      target: production-etl
    container_name: pwc-etl-gold-prod
    environment:
      ENVIRONMENT: production
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      SPARK_MASTER_URL: spark://spark-master:7077
      PROCESSING_ENGINE: spark
      LOG_LEVEL: INFO
    volumes:
      - ./data/prod:/app/data:ro
      - ./logs/prod:/app/logs
    restart: "no"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  # =============================================================================
  # ORCHESTRATION - PRODUCTION CONFIGURATION
  # =============================================================================
  dagster:
    build:
      context: .
      dockerfile: docker/Dockerfile.production
      target: production-dagster
    container_name: pwc-dagster-prod
    environment:
      ENVIRONMENT: production
      DAGSTER_POSTGRES_DB: ${POSTGRES_DB}
      DAGSTER_POSTGRES_USER: ${POSTGRES_USER}
      DAGSTER_POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      LOG_LEVEL: INFO
    ports:
      - "${DAGSTER_PORT}:3000"
    volumes:
      - ./data/prod:/app/data:ro
      - ./logs/prod:/app/logs
      - dagster_prod_home:/app/dagster_home
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/server_info"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
    security_opt:
      - no-new-privileges:true

  # =============================================================================
  # REVERSE PROXY & LOAD BALANCER
  # =============================================================================
  nginx:
    build:
      context: .
      dockerfile: docker/Dockerfile.production
      target: production-nginx
    container_name: pwc-nginx-prod
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx/prod.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/ssl/certs:ro
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - api
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "5"
    security_opt:
      - no-new-privileges:true
    networks:
      - frontend-network
      - backend-network

  # =============================================================================
  # BACKUP SERVICES
  # =============================================================================
  backup-service:
    build:
      context: .
      dockerfile: docker/Dockerfile.backup
    container_name: pwc-backup-service-prod
    environment:
      ENVIRONMENT: production
      POSTGRES_HOST: postgres
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      BACKUP_SCHEDULE: ${BACKUP_SCHEDULE:-0 2 * * *}  # Daily at 2 AM
      BACKUP_RETENTION_DAYS: ${BACKUP_RETENTION_DAYS:-30}
      S3_BUCKET: ${BACKUP_S3_BUCKET}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    volumes:
      - ./backups:/app/backups
      - postgres_prod_data:/var/lib/postgresql/data:ro
      - rabbitmq_prod_data:/var/lib/rabbitmq:ro
      - kafka_prod_data:/var/lib/kafka/data:ro
      - typesense_prod_data:/data:ro
    depends_on:
      - postgres
      - rabbitmq
      - kafka
      - typesense
    restart: unless-stopped
    profiles: ["backup"]
    networks:
      - database-network

volumes:
  # Production-specific volumes with backup labels
  postgres_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/postgres
  rabbitmq_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/rabbitmq
  zookeeper_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/zookeeper/data
  zookeeper_prod_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/zookeeper/logs
  kafka_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/kafka
  kafka_2_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/kafka-2
  kafka_3_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/kafka-3
  spark_prod_recovery:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/spark/recovery
  spark_prod_warehouse:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/spark/warehouse
  typesense_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/typesense
  dagster_prod_home:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/dagster
  prometheus_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/prometheus
  grafana_prod_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH}/grafana

networks:
  # Production networks with security configurations
  frontend-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
    driver_opts:
      com.docker.network.bridge.name: pwc-frontend
      
  backend-network:
    driver: bridge
    internal: false  # Allow external access for API
    ipam:
      config:
        - subnet: 172.21.0.0/24
    driver_opts:
      com.docker.network.bridge.name: pwc-backend
        
  database-network:
    driver: bridge
    internal: true  # No external access
    ipam:
      config:
        - subnet: 172.22.0.0/24
    driver_opts:
      com.docker.network.bridge.name: pwc-database
        
  monitoring-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.23.0.0/24
    driver_opts:
      com.docker.network.bridge.name: pwc-monitoring