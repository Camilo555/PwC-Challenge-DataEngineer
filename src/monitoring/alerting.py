"""
Advanced Alerting System
Provides multi-channel alerting with escalation policies and intelligent filtering
"""
from __future__ import annotations

import json
import queue
import smtplib
import threading
from collections.abc import Callable
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any

import requests

try:
    from email.mime.multipart import MimeMultipart
    from email.mime.text import MimeText
    EMAIL_AVAILABLE = True
except ImportError:
    EMAIL_AVAILABLE = False

from core.config import settings
from core.logging import get_logger

logger = get_logger(__name__)


class AlertSeverity(Enum):
    """Alert severity levels."""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class AlertChannel(Enum):
    """Alert delivery channels."""
    EMAIL = "email"
    SLACK = "slack"
    WEBHOOK = "webhook"
    SMS = "sms"
    CONSOLE = "console"


@dataclass
class Alert:
    """Represents an alert."""
    id: str
    title: str
    description: str
    severity: AlertSeverity
    source: str
    timestamp: datetime
    tags: dict[str, str]
    resolved: bool = False
    resolved_at: datetime | None = None
    acknowledged: bool = False
    acknowledged_by: str | None = None
    acknowledged_at: datetime | None = None


@dataclass
class AlertRule:
    """Defines conditions for triggering alerts."""
    name: str
    condition: Callable[[dict[str, Any]], bool]
    severity: AlertSeverity
    channels: list[AlertChannel]
    cooldown_minutes: int = 30
    enabled: bool = True
    description: str = ""


class AlertManager:
    """Manages alert generation, routing, and delivery."""

    def __init__(self):
        self.rules: dict[str, AlertRule] = {}
        self.alerts: dict[str, Alert] = {}
        self.alert_queue = queue.Queue()
        self.last_triggered = {}  # Track last trigger time for cooldowns
        self.notification_thread = None
        self.is_running = False

        # Configuration
        self.smtp_config = self._get_smtp_config()
        self.slack_config = self._get_slack_config()

        # Initialize default rules
        self._setup_default_rules()

    def _get_smtp_config(self) -> dict[str, Any]:
        """Get SMTP configuration from settings."""
        return {
            'host': getattr(settings, 'smtp_host', 'localhost'),
            'port': getattr(settings, 'smtp_port', 587),
            'user': getattr(settings, 'smtp_user', ''),
            'password': getattr(settings, 'smtp_password', ''),
            'from_email': getattr(settings, 'alert_email_from', 'alerts@example.com'),
            'to_emails': getattr(settings, 'alert_email_to', [])
        }

    def _get_slack_config(self) -> dict[str, Any]:
        """Get Slack configuration from settings."""
        return {
            'webhook_url': getattr(settings, 'slack_webhook_url', ''),
            'channel': getattr(settings, 'slack_channel', '#alerts'),
            'username': 'ETL Monitor',
            'icon_emoji': ':warning:'
        }

    def _setup_default_rules(self):
        """Setup default alerting rules."""
        # System resource alerts
        self.add_rule(AlertRule(
            name="high_cpu_usage",
            condition=lambda data: data.get('cpu_usage_percent', 0) > 90,
            severity=AlertSeverity.HIGH,
            channels=[AlertChannel.EMAIL, AlertChannel.SLACK],
            cooldown_minutes=15,
            description="CPU usage above 90%"
        ))

        self.add_rule(AlertRule(
            name="high_memory_usage",
            condition=lambda data: data.get('memory_usage_percent', 0) > 95,
            severity=AlertSeverity.CRITICAL,
            channels=[AlertChannel.EMAIL, AlertChannel.SLACK],
            cooldown_minutes=10,
            description="Memory usage above 95%"
        ))

        self.add_rule(AlertRule(
            name="disk_space_critical",
            condition=lambda data: data.get('disk_usage_percent', 0) > 95,
            severity=AlertSeverity.CRITICAL,
            channels=[AlertChannel.EMAIL, AlertChannel.SLACK],
            cooldown_minutes=30,
            description="Disk usage above 95%"
        ))

        # ETL pipeline alerts
        self.add_rule(AlertRule(
            name="etl_failure_rate_high",
            condition=lambda data: (
                data.get('records_failed', 0) / max(data.get('records_processed', 1), 1)
            ) > 0.1,
            severity=AlertSeverity.HIGH,
            channels=[AlertChannel.EMAIL, AlertChannel.SLACK],
            cooldown_minutes=20,
            description="ETL failure rate above 10%"
        ))

        self.add_rule(AlertRule(
            name="data_quality_low",
            condition=lambda data: data.get('data_quality_score', 100) < 70,
            severity=AlertSeverity.MEDIUM,
            channels=[AlertChannel.EMAIL],
            cooldown_minutes=60,
            description="Data quality score below 70%"
        ))

        self.add_rule(AlertRule(
            name="processing_time_slow",
            condition=lambda data: data.get('processing_time_seconds', 0) > 600,
            severity=AlertSeverity.MEDIUM,
            channels=[AlertChannel.SLACK],
            cooldown_minutes=30,
            description="Processing time over 10 minutes"
        ))

    def start(self):
        """Start the alert manager."""
        if self.is_running:
            logger.warning("Alert manager already running")
            return

        self.is_running = True
        self.notification_thread = threading.Thread(target=self._notification_worker, daemon=True)
        self.notification_thread.start()
        logger.info("Alert manager started")

    def stop(self):
        """Stop the alert manager."""
        self.is_running = False
        if self.notification_thread:
            self.notification_thread.join(timeout=5)
        logger.info("Alert manager stopped")

    def add_rule(self, rule: AlertRule):
        """Add an alerting rule."""
        self.rules[rule.name] = rule
        logger.info(f"Added alert rule: {rule.name}")

    def remove_rule(self, rule_name: str):
        """Remove an alerting rule."""
        if rule_name in self.rules:
            del self.rules[rule_name]
            logger.info(f"Removed alert rule: {rule_name}")

    def evaluate_metrics(self, metrics_data: dict[str, Any], source: str = "system"):
        """Evaluate metrics against all rules and trigger alerts."""
        for rule_name, rule in self.rules.items():
            if not rule.enabled:
                continue

            try:
                # Check cooldown
                if self._is_in_cooldown(rule_name):
                    continue

                # Evaluate condition
                if rule.condition(metrics_data):
                    self._trigger_alert(rule, metrics_data, source)

            except Exception as e:
                logger.error(f"Error evaluating rule {rule_name}: {e}")

    def _is_in_cooldown(self, rule_name: str) -> bool:
        """Check if rule is in cooldown period."""
        if rule_name not in self.last_triggered:
            return False

        rule = self.rules[rule_name]
        last_trigger = self.last_triggered[rule_name]
        cooldown_period = timedelta(minutes=rule.cooldown_minutes)

        return datetime.now() - last_trigger < cooldown_period

    def _trigger_alert(self, rule: AlertRule, metrics_data: dict[str, Any], source: str):
        """Trigger an alert based on rule and metrics."""
        alert_id = f"{rule.name}_{int(datetime.now().timestamp())}"

        # Extract relevant context from metrics
        context_info = []
        if 'cpu_usage_percent' in metrics_data:
            context_info.append(f"CPU: {metrics_data['cpu_usage_percent']:.1f}%")
        if 'memory_usage_percent' in metrics_data:
            context_info.append(f"Memory: {metrics_data['memory_usage_percent']:.1f}%")
        if 'processing_time_seconds' in metrics_data:
            context_info.append(f"Processing time: {metrics_data['processing_time_seconds']:.1f}s")
        if 'data_quality_score' in metrics_data:
            context_info.append(f"Quality score: {metrics_data['data_quality_score']:.1f}")

        context = " | ".join(context_info)
        description = f"{rule.description}"
        if context:
            description += f" | {context}"

        alert = Alert(
            id=alert_id,
            title=f"Alert: {rule.name}",
            description=description,
            severity=rule.severity,
            source=source,
            timestamp=datetime.now(),
            tags={"rule": rule.name, "source": source}
        )

        self.alerts[alert_id] = alert
        self.last_triggered[rule.name] = datetime.now()

        # Queue alert for delivery
        for channel in rule.channels:
            self.alert_queue.put((alert, channel))

        logger.warning(f"ALERT TRIGGERED [{rule.severity.value.upper()}]: {alert.title} - {alert.description}")

    def _notification_worker(self):
        """Background worker for sending notifications."""
        while self.is_running:
            try:
                # Get alert from queue with timeout
                alert, channel = self.alert_queue.get(timeout=1)

                # Send notification
                success = self._send_notification(alert, channel)

                if success:
                    logger.info(f"Alert notification sent via {channel.value}: {alert.title}")
                else:
                    logger.error(f"Failed to send alert via {channel.value}: {alert.title}")

                self.alert_queue.task_done()

            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"Error in notification worker: {e}")

    def _send_notification(self, alert: Alert, channel: AlertChannel) -> bool:
        """Send alert notification via specified channel."""
        try:
            if channel == AlertChannel.EMAIL:
                return self._send_email_alert(alert)
            elif channel == AlertChannel.SLACK:
                return self._send_slack_alert(alert)
            elif channel == AlertChannel.WEBHOOK:
                return self._send_webhook_alert(alert)
            elif channel == AlertChannel.CONSOLE:
                return self._send_console_alert(alert)
            else:
                logger.warning(f"Unsupported alert channel: {channel}")
                return False

        except Exception as e:
            logger.error(f"Failed to send {channel.value} notification: {e}")
            return False

    def _send_email_alert(self, alert: Alert) -> bool:
        """Send alert via email."""
        if not EMAIL_AVAILABLE:
            logger.warning("Email functionality not available")
            return False

        if not self.smtp_config['to_emails']:
            logger.warning("No email recipients configured")
            return False

        try:
            # Create message
            msg = MimeMultipart()
            msg['From'] = self.smtp_config['from_email']
            msg['To'] = ', '.join(self.smtp_config['to_emails'])
            msg['Subject'] = f"[{alert.severity.value.upper()}] {alert.title}"

            # Email body
            body = f"""
Alert Details:
=============
Title: {alert.title}
Severity: {alert.severity.value.upper()}
Source: {alert.source}
Time: {alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')}
Description: {alert.description}

Alert ID: {alert.id}

This is an automated alert from the ETL monitoring system.
"""

            msg.attach(MimeText(body, 'plain'))

            # Send email
            server = smtplib.SMTP(self.smtp_config['host'], self.smtp_config['port'])

            if self.smtp_config['user'] and self.smtp_config['password']:
                server.starttls()
                server.login(self.smtp_config['user'], self.smtp_config['password'])

            server.send_message(msg)
            server.quit()

            return True

        except Exception as e:
            logger.error(f"Failed to send email alert: {e}")
            return False

    def _send_slack_alert(self, alert: Alert) -> bool:
        """Send alert via Slack webhook."""
        if not self.slack_config['webhook_url']:
            logger.warning("Slack webhook URL not configured")
            return False

        try:
            # Color coding for severity
            colors = {
                AlertSeverity.CRITICAL: "danger",
                AlertSeverity.HIGH: "warning",
                AlertSeverity.MEDIUM: "#ff9500",
                AlertSeverity.LOW: "good",
                AlertSeverity.INFO: "#439FE0"
            }

            # Create Slack message
            payload = {
                "channel": self.slack_config['channel'],
                "username": self.slack_config['username'],
                "icon_emoji": self.slack_config['icon_emoji'],
                "attachments": [{
                    "color": colors.get(alert.severity, "warning"),
                    "title": alert.title,
                    "text": alert.description,
                    "fields": [
                        {"title": "Severity", "value": alert.severity.value.upper(), "short": True},
                        {"title": "Source", "value": alert.source, "short": True},
                        {"title": "Time", "value": alert.timestamp.strftime('%Y-%m-%d %H:%M:%S'), "short": True},
                        {"title": "Alert ID", "value": alert.id, "short": True}
                    ],
                    "footer": "ETL Monitoring System",
                    "ts": int(alert.timestamp.timestamp())
                }]
            }

            response = requests.post(
                self.slack_config['webhook_url'],
                json=payload,
                timeout=10
            )

            return response.status_code == 200

        except Exception as e:
            logger.error(f"Failed to send Slack alert: {e}")
            return False

    def _send_webhook_alert(self, alert: Alert) -> bool:
        """Send alert via generic webhook."""
        # This would be implemented based on specific webhook requirements
        logger.info(f"Webhook alert: {alert.title}")
        return True

    def _send_console_alert(self, alert: Alert) -> bool:
        """Send alert to console/logs."""
        severity_symbols = {
            AlertSeverity.CRITICAL: "ðŸ”´",
            AlertSeverity.HIGH: "ðŸŸ ",
            AlertSeverity.MEDIUM: "ðŸŸ¡",
            AlertSeverity.LOW: "ðŸ”µ",
            AlertSeverity.INFO: "â„¹ï¸"
        }

        symbol = severity_symbols.get(alert.severity, "âš ï¸")
        print(f"\n{symbol} ALERT [{alert.severity.value.upper()}]: {alert.title}")
        print(f"   Description: {alert.description}")
        print(f"   Source: {alert.source}")
        print(f"   Time: {alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   ID: {alert.id}\n")

        return True

    def acknowledge_alert(self, alert_id: str, acknowledged_by: str = "system") -> bool:
        """Acknowledge an alert."""
        if alert_id in self.alerts:
            alert = self.alerts[alert_id]
            alert.acknowledged = True
            alert.acknowledged_by = acknowledged_by
            alert.acknowledged_at = datetime.now()

            logger.info(f"Alert acknowledged: {alert_id} by {acknowledged_by}")
            return True

        return False

    def resolve_alert(self, alert_id: str) -> bool:
        """Mark an alert as resolved."""
        if alert_id in self.alerts:
            alert = self.alerts[alert_id]
            alert.resolved = True
            alert.resolved_at = datetime.now()

            logger.info(f"Alert resolved: {alert_id}")
            return True

        return False

    def get_active_alerts(self) -> list[Alert]:
        """Get all active (unresolved) alerts."""
        return [alert for alert in self.alerts.values() if not alert.resolved]

    def get_alert_summary(self, hours: int = 24) -> dict[str, Any]:
        """Get alert summary for the last N hours."""
        cutoff_time = datetime.now() - timedelta(hours=hours)

        recent_alerts = [
            alert for alert in self.alerts.values()
            if alert.timestamp > cutoff_time
        ]

        # Count by severity
        severity_counts = {}
        for severity in AlertSeverity:
            severity_counts[severity.value] = sum(
                1 for alert in recent_alerts if alert.severity == severity
            )

        # Count by source
        source_counts = {}
        for alert in recent_alerts:
            source_counts[alert.source] = source_counts.get(alert.source, 0) + 1

        return {
            'period_hours': hours,
            'total_alerts': len(recent_alerts),
            'active_alerts': len([a for a in recent_alerts if not a.resolved]),
            'severity_breakdown': severity_counts,
            'source_breakdown': source_counts,
            'most_recent': recent_alerts[-1].title if recent_alerts else None,
            'generated_at': datetime.now().isoformat()
        }

    def export_alerts(self, output_file: Path, hours: int = 24) -> bool:
        """Export alerts to JSON file."""
        try:
            cutoff_time = datetime.now() - timedelta(hours=hours)

            recent_alerts = [
                {
                    **asdict(alert),
                    'timestamp': alert.timestamp.isoformat(),
                    'resolved_at': alert.resolved_at.isoformat() if alert.resolved_at else None,
                    'acknowledged_at': alert.acknowledged_at.isoformat() if alert.acknowledged_at else None,
                    'severity': alert.severity.value
                }
                for alert in self.alerts.values()
                if alert.timestamp > cutoff_time
            ]

            export_data = {
                'summary': self.get_alert_summary(hours),
                'alerts': recent_alerts
            }

            with open(output_file, 'w') as f:
                json.dump(export_data, f, indent=2)

            logger.info(f"Alerts exported to {output_file}")
            return True

        except Exception as e:
            logger.error(f"Failed to export alerts: {e}")
            return False


# Global alert manager instance
_alert_manager = None


def get_alert_manager() -> AlertManager:
    """Get global alert manager instance."""
    global _alert_manager
    if _alert_manager is None:
        _alert_manager = AlertManager()
    return _alert_manager


def trigger_custom_alert(title: str, description: str, severity: AlertSeverity,
                        source: str = "custom", tags: dict[str, str] | None = None):
    """Trigger a custom alert."""
    manager = get_alert_manager()

    alert_id = f"custom_{int(datetime.now().timestamp())}"
    alert = Alert(
        id=alert_id,
        title=title,
        description=description,
        severity=severity,
        source=source,
        timestamp=datetime.now(),
        tags=tags or {}
    )

    manager.alerts[alert_id] = alert

    # Send to console by default
    manager.alert_queue.put((alert, AlertChannel.CONSOLE))

    logger.warning(f"CUSTOM ALERT [{severity.value.upper()}]: {title} - {description}")


def main():
    """Test the alerting system."""
    print("Alerting System Module loaded successfully")
    print("Available features:")
    print("- Multi-channel alerts (Email, Slack, Webhook, Console)")
    print("- Configurable severity levels")
    print("- Alert rules with conditions and cooldowns")
    print("- Alert acknowledgment and resolution")
    print("- Alert history and reporting")
    print("- Background notification processing")


if __name__ == "__main__":
    main()
