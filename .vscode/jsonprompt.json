{
    "version": "8.0-ProductionReady-EN",
    "name": "Senior Data Architect & Engineering Expert Guide - Complete Implementation",
    "project_metadata": {
        "project_name": "de-challenge-retail-etl",
        "repository_name": "data-engineering-challenge-2025",
        "python_version": "3.10.12",
        "estimated_duration": "40-60 hours",
        "complexity_level": "Senior",
        "github_visibility": "public"
    },
    "critical_requirements_summary": {
        "absolutely_mandatory": [
            "PySpark ONLY for ETL transformations (pandas/polars acceptable as alternatives)",
            "SQL/SQLAlchemy ONLY for schema creation, NOT for data transformations",
            "Pydantic models for domain entities (business logic)",
            "SQLModel for data access layer (database tables)",
            "Star schema: minimum 1 fact table + 5 dimension tables",
            "Typesense vector search with AT LEAST ONE filter",
            "FastAPI with layered architecture (4 layers minimum)",
            "Basic Auth on ALL endpoints",
            "Docker Compose for orchestration",
            "Mermaid diagrams for EACH medallion layer + overall architecture",
            "Type hints on ALL code",
            "GitHub public repository"
        ],
        "critical_distinctions": {
            "etl_vs_sql": "ETL transformations MUST use DataFrame API (PySpark/Pandas/Polars), NOT SQL queries",
            "domain_vs_data": "Domain entities (Pydantic) ≠ Data Access entities (SQLModel) - MUST be separate",
            "schema_creation": "API must auto-create schemas if they don't exist on startup",
            "data_types": "Must handle CSV, JSON, and PDF files"
        }
    },
    "technical_architecture": {
        "medallion_layers": {
            "bronze": {
                "purpose": "Raw data ingestion with metadata",
                "storage": "Delta Lake or Parquet files",
                "transformations": "NONE - only add metadata",
                "schema": "Schema-on-read, preserve original structure",
                "metadata_columns": [
                    "ingestion_timestamp",
                    "source_file_path",
                    "source_file_type",
                    "source_file_size",
                    "ingestion_job_id",
                    "raw_record_count"
                ],
                "implementation": {
                    "structured_data": "spark.read.option('header', 'true').option('inferSchema', 'true')",
                    "unstructured_data": "PyPDF2 for text extraction → DataFrame conversion",
                    "partitioning": "By ingestion_date (YYYY-MM-DD format)"
                }
            },
            "silver": {
                "purpose": "Cleaned, validated, deduplicated data",
                "storage": "Delta Lake or Parquet files",
                "transformations": [
                    "Data type standardization",
                    "Null handling",
                    "Deduplication",
                    "Business validation with Pydantic",
                    "Data quality checks"
                ],
                "validation_implementation": {
                    "method": "Pandas UDF with Pydantic models",
                    "error_handling": "Separate error column for validation failures",
                    "quarantine": "Invalid records to separate location"
                }
            },
            "gold": {
                "purpose": "Business-ready star schema",
                "storage": "SQLite database (initially) → PostgreSQL-ready",
                "star_schema": {
                    "fact_table": {
                        "name": "fact_sales",
                        "grain": "One row per product per invoice",
                        "columns": [
                            "sale_id (PK)",
                            "product_key (FK)",
                            "customer_key (FK)",
                            "date_key (FK)",
                            "invoice_key (FK)",
                            "country_key (FK)",
                            "quantity",
                            "unit_price",
                            "total_amount",
                            "discount_amount"
                        ]
                    },
                    "dimension_tables": [
                        {
                            "name": "dim_product",
                            "columns": [
                                "product_key (PK)",
                                "stock_code",
                                "description",
                                "category",
                                "subcategory",
                                "brand"
                            ]
                        },
                        {
                            "name": "dim_customer",
                            "columns": [
                                "customer_key (PK)",
                                "customer_id",
                                "customer_segment",
                                "registration_date",
                                "lifetime_value"
                            ]
                        },
                        {
                            "name": "dim_date",
                            "columns": [
                                "date_key (PK)",
                                "date",
                                "year",
                                "quarter",
                                "month",
                                "week",
                                "day_name",
                                "is_weekend",
                                "is_holiday"
                            ]
                        },
                        {
                            "name": "dim_country",
                            "columns": [
                                "country_key (PK)",
                                "country_code",
                                "country_name",
                                "region",
                                "continent"
                            ]
                        },
                        {
                            "name": "dim_invoice",
                            "columns": [
                                "invoice_key (PK)",
                                "invoice_no",
                                "invoice_date",
                                "is_cancelled",
                                "payment_method"
                            ]
                        }
                    ]
                }
            }
        }
    },
    "implementation_details": {
        "etl_framework_specifics": {
            "pyspark_configuration": {
                "spark.app.name": "RetailETL",
                "spark.master": "local[*]",
                "spark.driver.memory": "4g",
                "spark.executor.memory": "4g",
                "spark.sql.adaptive.enabled": "true",
                "spark.sql.extensions": "io.delta.sql.DeltaSparkSessionExtension",
                "spark.sql.catalog.spark_catalog": "org.apache.spark.sql.delta.catalog.DeltaCatalog"
            },
            "transformation_patterns": {
                "bronze_to_silver": [
                    "df.filter(col('quantity').isNotNull())",
                    "df.withColumn('invoice_date', to_timestamp('InvoiceDate', 'MM/dd/yyyy HH:mm'))",
                    "df.dropDuplicates(['Invoice', 'StockCode', 'CustomerID'])",
                    "df.withColumn('is_cancelled', col('Invoice').startswith('C'))"
                ],
                "silver_to_gold": [
                    "Build dimensions first using window functions for surrogate keys",
                    "Use broadcast joins for dimension lookups",
                    "Aggregate facts at proper grain",
                    "Write to SQLite using JDBC: df.write.jdbc(url, table, mode='overwrite')"
                ]
            }
        },
        "api_layer_specifics": {
            "layered_architecture": {
                "layer_1_routes": {
                    "location": "src/de_challenge/api/v1/routes/",
                    "responsibility": "HTTP request handling, input validation",
                    "example_endpoints": [
                        "POST /api/v1/seed - Initial data seeding",
                        "GET /api/v1/sales - Query fact table",
                        "POST /api/v1/raw-data - Create raw data entry",
                        "GET /api/v1/raw-data/{id} - Read raw data",
                        "PUT /api/v1/raw-data/{id} - Update raw data",
                        "DELETE /api/v1/raw-data/{id} - Delete raw data",
                        "POST /api/v1/raw-data/batch - Batch operations",
                        "GET /api/v1/search - Vector search with filters"
                    ]
                },
                "layer_2_services": {
                    "location": "src/de_challenge/api/v1/services/",
                    "responsibility": "Business logic orchestration",
                    "patterns": [
                        "Unit of Work",
                        "Command Query Separation"
                    ]
                },
                "layer_3_domain": {
                    "location": "src/de_challenge/domain/",
                    "responsibility": "Business entities and validation rules",
                    "implementation": "Pydantic BaseModel classes with validators"
                },
                "layer_4_data_access": {
                    "location": "src/de_challenge/data_access/",
                    "responsibility": "Database interaction",
                    "implementation": "SQLModel for ORM, Repository pattern"
                }
            },
            "authentication": {
                "basic_auth_implementation": {
                    "dependency": "from fastapi.security import HTTPBasic, HTTPBasicCredentials",
                    "validation": "secrets.compare_digest for timing attack prevention",
                    "application": "Depends(get_current_user) on all routes"
                }
            }
        },
        "vector_search_specifics": {
            "typesense_configuration": {
                "collection_schema": {
                    "name": "products",
                    "fields": [
                        {
                            "name": "id",
                            "type": "string"
                        },
                        {
                            "name": "stock_code",
                            "type": "string",
                            "facet": true
                        },
                        {
                            "name": "description",
                            "type": "string"
                        },
                        {
                            "name": "description_vector",
                            "type": "float[]",
                            "num_dim": 384
                        },
                        {
                            "name": "category",
                            "type": "string",
                            "facet": true
                        },
                        {
                            "name": "price_range",
                            "type": "string",
                            "facet": true
                        },
                        {
                            "name": "country",
                            "type": "string[]",
                            "facet": true
                        }
                    ]
                },
                "embedding_generation": {
                    "model": "sentence-transformers/all-MiniLM-L6-v2",
                    "batch_size": 32,
                    "dimension": 384
                },
                "search_implementation": {
                    "required_filter": "At least ONE filter must be implemented",
                    "filter_examples": [
                        "filter_by: 'category:electronics'",
                        "filter_by: 'price_range:[10 TO 100]'",
                        "filter_by: 'country:=[\"UK\", \"USA\"]'"
                    ]
                }
            }
        }
    },
    "docker_configuration": {
        "docker_compose_services": {
            "api": {
                "build": {
                    "context": ".",
                    "dockerfile": "Dockerfile"
                },
                "ports": [
                    "8000:8000"
                ],
                "environment": {
                    "DATABASE_URL": "sqlite:///./data/warehouse/retail.db",
                    "TYPESENSE_API_KEY": "${TYPESENSE_API_KEY}",
                    "TYPESENSE_HOST": "typesense"
                },
                "volumes": [
                    "./data:/app/data",
                    "./logs:/app/logs"
                ],
                "depends_on": [
                    "typesense"
                ],
                "healthcheck": {
                    "test": "curl -f http://localhost:8000/health || exit 1",
                    "interval": "30s",
                    "timeout": "10s",
                    "retries": 3
                }
            },
            "typesense": {
                "image": "typesense/typesense:0.25.1",
                "ports": [
                    "8108:8108"
                ],
                "volumes": [
                    "typesense-data:/data"
                ],
                "environment": {
                    "TYPESENSE_DATA_DIR": "/data",
                    "TYPESENSE_API_KEY": "${TYPESENSE_API_KEY}"
                }
            }
        },
        "dockerfile_multi_stage": {
            "stage_1_builder": [
                "FROM python:3.10-slim as builder",
                "RUN pip install poetry",
                "COPY pyproject.toml poetry.lock ./",
                "RUN poetry export -f requirements.txt > requirements.txt"
            ],
            "stage_2_runtime": [
                "FROM python:3.10-slim",
                "COPY --from=builder requirements.txt .",
                "RUN pip install -r requirements.txt",
                "COPY src/ /app/src/",
                "CMD [\"uvicorn\", \"src.de_challenge.main:app\", \"--host\", \"0.0.0.0\"]"
            ]
        }
    },
    "mermaid_diagrams": {
        "bronze_layer": "```mermaid\ngraph TB\n    subgraph Input\n        CSV[CSV Files]\n        JSON[JSON Files]\n        PDF[PDF Files]\n    end\n    subgraph Bronze Processing\n        Reader[PySpark Reader]\n        Meta[Add Metadata]\n        Part[Partition by Date]\n    end\n    subgraph Bronze Storage\n        Delta[(Delta Lake/Parquet)]\n    end\n    CSV --> Reader\n    JSON --> Reader\n    PDF --> Reader\n    Reader --> Meta\n    Meta --> Part\n    Part --> Delta\n```",
        "silver_layer": "```mermaid\ngraph TB\n    Bronze[(Bronze Layer)] --> Read[Read with PySpark]\n    Read --> Clean[Clean & Standardize]\n    Clean --> Validate{Pydantic Validation}\n    Validate -->|Valid| Dedupe[Deduplicate]\n    Validate -->|Invalid| Quarantine[(Quarantine)]\n    Dedupe --> Transform[Apply Business Rules]\n    Transform --> Silver[(Silver Layer)]\n```",
        "gold_layer": "```mermaid\nflowchart TB\n    Silver[(Silver Layer)] --> Process[PySpark Processing]\n    Process --> Dims[Build Dimensions]\n    Process --> Facts[Build Facts]\n    Dims --> DP[dim_product]\n    Dims --> DC[dim_customer]\n    Dims --> DD[dim_date]\n    Dims --> DCO[dim_country]\n    Dims --> DI[dim_invoice]\n    Facts --> FS[fact_sales]\n    DP & DC & DD & DCO & DI --> Join[Join with Facts]\n    Join --> FS\n    FS --> Load[JDBC Load]\n    Load --> DB[(SQLite/PostgreSQL)]\n```",
        "overall_architecture": "```mermaid\ngraph TB\n    subgraph Data Sources\n        Files[CSV/JSON/PDF]\n    end\n    subgraph Medallion Architecture\n        Bronze[Bronze - Raw]\n        Silver[Silver - Cleaned]\n        Gold[Gold - Star Schema]\n    end\n    subgraph Storage\n        Lake[(Delta Lake)]\n        Warehouse[(SQLite)]\n    end\n    subgraph Vector Search\n        Embed[Embeddings]\n        Typesense[(Typesense)]\n    end\n    subgraph API Layer\n        Routes[Routes]\n        Services[Services]\n        Domain[Domain]\n        DataAccess[Data Access]\n    end\n    Files -->|PySpark| Bronze\n    Bronze -->|PySpark| Silver\n    Silver -->|PySpark| Gold\n    Bronze & Silver --> Lake\n    Gold -->|JDBC| Warehouse\n    Warehouse --> DataAccess\n    Domain --> Services\n    Services --> Routes\n    DataAccess --> Services\n    Warehouse -->|Sync| Embed\n    Embed --> Typesense\n    Typesense --> Services\n    Routes -->|FastAPI| Client[Client]\n```"
    },
    "repository_structure": {
        "root_files": [
            "README.md - Setup instructions and documentation",
            "pyproject.toml - Poetry configuration with dependencies",
            "poetry.lock - Locked dependencies",
            ".gitignore - Python, Spark, Delta, IDE, data folders",
            ".env.example - Environment variables template",
            "Makefile - Common commands",
            "docker-compose.yml - Service orchestration",
            "Dockerfile - Multi-stage build"
        ],
        "source_structure": {
            "src/de_challenge/": {
                "__init__.py": "Package initialization",
                "main.py": "FastAPI application entry point",
                "api/": "API layer implementation",
                "core/": "Configuration and utilities",
                "domain/": "Business entities (Pydantic)",
                "data_access/": "Database layer (SQLModel)",
                "etl/": "PySpark ETL pipelines",
                "vector_search/": "Typesense integration"
            }
        }
    },
    "dependencies": {
        "production": [
            "pyspark>=3.5.0",
            "delta-spark>=3.0.0",
            "fastapi>=0.104.0",
            "uvicorn[standard]>=0.24.0",
            "pydantic>=2.5.0",
            "pydantic-settings>=2.1.0",
            "sqlmodel>=0.0.14",
            "typesense>=0.19.0",
            "pandas>=2.1.0",
            "pypdf2>=3.0.0",
            "python-multipart>=0.0.6",
            "sentence-transformers>=2.2.0",
            "python-jose[cryptography]>=3.3.0",
            "passlib[bcrypt]>=1.7.4"
        ],
        "development": [
            "pytest>=7.4.0",
            "pytest-asyncio>=0.21.0",
            "pytest-cov>=4.1.0",
            "ruff>=0.1.0",
            "black>=23.0.0",
            "mypy>=1.7.0",
            "pre-commit>=3.5.0",
            "httpx>=0.25.0"
        ]
    },
    "optional_features_recommendations": {
        "already_covered": [
            "Feature #4: PySpark for ETL (mandatory, not optional)",
            "Feature #20: Poetry for dependency locking"
        ],
        "recommended_to_implement": [
            {
                "feature": "#6: Alembic migrations",
                "reason": "Essential for schema versioning and Supabase migration",
                "effort": "Low"
            },
            {
                "feature": "#22: Logging with loguru",
                "reason": "Better debugging and monitoring",
                "effort": "Low"
            },
            {
                "feature": "#28-29: Unit and integration tests",
                "reason": "Code quality and reliability",
                "effort": "Medium"
            }
        ]
    },
    "common_pitfalls_to_avoid": [
        "❌ Using SQL for ETL transformations instead of DataFrame API",
        "❌ Mixing Pydantic domain models with SQLModel database models",
        "❌ Forgetting to implement filter in vector search",
        "❌ Missing authentication on endpoints",
        "❌ Not creating all 5 dimension tables",
        "❌ Using wrong technology for ETL (must be pandas/polars/pyspark)",
        "❌ Not handling PDF files in ingestion",
        "❌ Missing batch operations in CRUD",
        "❌ Forgetting Mermaid diagrams",
        "❌ Not using type hints everywhere"
    ],
    "testing_strategy": {
        "unit_tests": {
            "focus_areas": [
                "Domain model validation",
                "ETL transformation logic",
                "Service layer business logic",
                "Repository CRUD operations"
            ],
            "tools": "pytest, pytest-mock",
            "coverage_target": "80%"
        },
        "integration_tests": {
            "api_tests": [
                "Authentication flow",
                "CRUD operations",
                "Batch operations",
                "Search with filters"
            ],
            "etl_tests": [
                "Bronze ingestion",
                "Silver validation",
                "Gold star schema build"
            ],
            "tools": "pytest-asyncio, httpx"
        }
    },
    "submission_checklist": {
        "code_requirements": [
            "✓ Public GitHub repository",
            "✓ All code has type hints",
            "✓ Poetry with locked dependencies",
            "✓ Docker Compose working",
            "✓ README with setup instructions"
        ],
        "technical_requirements": [
            "✓ ETL using PySpark/Pandas/Polars (NOT SQL)",
            "✓ Star schema with 1 fact + 5 dimensions",
            "✓ Pydantic for domain models",
            "✓ SQLModel for database models",
            "✓ Domain ≠ Data Access entities"
        ],
        "api_requirements": [
            "✓ FastAPI with 4-layer architecture",
            "✓ Basic Auth on all endpoints",
            "✓ CRUD with batch operations",
            "✓ Vector search with at least 1 filter",
            "✓ Auto-create schemas on startup"
        ],
        "documentation": [
            "✓ Mermaid diagram for Bronze layer",
            "✓ Mermaid diagram for Silver layer",
            "✓ Mermaid diagram for Gold layer",
            "✓ Mermaid diagram for overall architecture",
            "✓ OpenAPI documentation available"
        ],
        "deployment": [
            "✓ Docker Compose with all services",
            "✓ Typesense running in container",
            "✓ Health checks configured",
            "✓ Environment variables properly managed"
        ],
        "bonus": [
            "✓ At least 1 optional feature implemented",
            "✓ Tests with good coverage",
            "✓ Clean code with linting",
            "✓ Performance optimizations"
        ]
    },
    "evaluation_criteria_alignment": {
        "database_design": "Star schema properly normalized with appropriate data types and relationships",
        "best_practices": "Clean architecture, SOLID principles, design patterns, error handling",
        "complexity": "Number and quality of optional features implemented",
        "organization": "Working solution delivered on time with clear structure",
        "innovation": "Creative solutions, performance optimizations, extra features"
    },
    "implementation_roadmap": {
        "phase_1_setup": {
            "duration": "4-6 hours",
            "tasks": [
                "Initialize Poetry project with all dependencies",
                "Set up project structure with 4-layer architecture",
                "Configure Docker Compose with Typesense",
                "Set up basic FastAPI application with health check"
            ]
        },
        "phase_2_etl_bronze": {
            "duration": "8-10 hours",
            "tasks": [
                "Implement PySpark session configuration",
                "Create bronze layer ingestion for CSV/JSON/PDF files",
                "Add metadata columns (ingestion_timestamp, source_file_path, etc.)",
                "Implement partitioning by ingestion_date",
                "Create Mermaid diagram for Bronze layer"
            ]
        },
        "phase_3_etl_silver": {
            "duration": "10-12 hours",
            "tasks": [
                "Create Pydantic domain models with validation",
                "Implement data cleaning and standardization",
                "Add deduplication logic",
                "Create error handling and quarantine system",
                "Create Mermaid diagram for Silver layer"
            ]
        },
        "phase_4_etl_gold": {
            "duration": "12-15 hours",
            "tasks": [
                "Design star schema with 1 fact + 5 dimension tables",
                "Create SQLModel database models",
                "Implement dimension building with surrogate keys",
                "Build fact table with proper grain",
                "Set up JDBC connection to SQLite",
                "Create Mermaid diagram for Gold layer"
            ]
        },
        "phase_5_api": {
            "duration": "10-12 hours",
            "tasks": [
                "Implement 4-layer FastAPI architecture",
                "Add Basic Authentication to all endpoints",
                "Create CRUD operations with batch support",
                "Implement schema auto-creation on startup",
                "Add comprehensive error handling"
            ]
        },
        "phase_6_vector_search": {
            "duration": "6-8 hours",
            "tasks": [
                "Set up sentence-transformers for embeddings",
                "Configure Typesense collection schema",
                "Implement vector embedding generation",
                "Create search endpoint with mandatory filters",
                "Test vector search functionality"
            ]
        },
        "phase_7_finalization": {
            "duration": "4-6 hours",
            "tasks": [
                "Create comprehensive README with setup instructions",
                "Add overall architecture Mermaid diagram",
                "Implement optional features (tests, logging, etc.)",
                "Final testing and deployment verification",
                "Create submission documentation"
            ]
        }
    },
    "critical_success_factors": {
        "data_processing": "ETL MUST use DataFrame API (PySpark/Pandas/Polars), never SQL for transformations",
        "architecture_separation": "Domain models (Pydantic) MUST be separate from Data Access models (SQLModel)",
        "star_schema_completeness": "Exactly 1 fact table + 5 dimension tables required",
        "authentication_coverage": "Basic Auth required on ALL API endpoints without exception",
        "vector_search_filters": "At least ONE filter must be implemented in search functionality",
        "file_type_support": "Must handle CSV, JSON, AND PDF files in ingestion pipeline",
        "schema_management": "API must auto-create database schemas if they don't exist",
        "containerization": "Docker Compose must orchestrate all services successfully"
    },
    "data_quality_framework": {
        "bronze_layer_checks": [
            "File existence and readability validation",
            "Basic schema inference and type detection",
            "Metadata completeness verification",
            "Ingestion timestamp consistency"
        ],
        "silver_layer_checks": [
            "Pydantic model validation for business rules",
            "Null value handling based on business logic",
            "Duplicate detection and resolution",
            "Data type standardization verification",
            "Range and format validations"
        ],
        "gold_layer_checks": [
            "Referential integrity between fact and dimensions",
            "Surrogate key uniqueness and consistency",
            "Fact table grain validation",
            "Dimension table completeness",
            "Business metric calculation accuracy"
        ]
    }
}