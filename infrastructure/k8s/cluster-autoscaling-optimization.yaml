# Advanced Cluster Auto-Scaling and Node Management
# Enterprise-grade cluster optimization with intelligent node provisioning
# Multi-tier node pools with cost-optimized scaling strategies

---
# Cluster Autoscaler Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::123456789012:role/cluster-autoscaler-role"
automountServiceAccountToken: true

---
# Cluster Autoscaler RBAC
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""]
  resources: ["events", "endpoints"]
  verbs: ["create", "patch"]
- apiGroups: [""]
  resources: ["pods/eviction"]
  verbs: ["create"]
- apiGroups: [""]
  resources: ["pods/status"]
  verbs: ["update"]
- apiGroups: [""]
  resources: ["endpoints"]
  resourceNames: ["cluster-autoscaler"]
  verbs: ["get", "update"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["watch", "list", "get", "update"]
- apiGroups: [""]
  resources:
  - "namespaces"
  - "pods"
  - "services"
  - "replicationcontrollers"
  - "persistentvolumeclaims"
  - "persistentvolumes"
  verbs: ["watch", "list", "get"]
- apiGroups: ["extensions"]
  resources: ["replicasets", "daemonsets"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["policy"]
  resources: ["poddisruptionbudgets"]
  verbs: ["watch", "list"]
- apiGroups: ["apps"]
  resources: ["statefulsets", "replicasets", "daemonsets"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities"]
  verbs: ["watch", "list", "get"]
- apiGroups: ["batch", "extensions"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "patch"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["create"]
- apiGroups: ["coordination.k8s.io"]
  resourceNames: ["cluster-autoscaler"]
  resources: ["leases"]
  verbs: ["get", "update"]

---
# Cluster Autoscaler RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
- kind: ServiceAccount
  name: cluster-autoscaler
  namespace: kube-system

---
# Cluster Autoscaler Role for kube-system
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["create","list","watch"]
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
  verbs: ["delete", "get", "update", "watch"]

---
# Cluster Autoscaler RoleBinding for kube-system
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
- kind: ServiceAccount
  name: cluster-autoscaler
  namespace: kube-system

---
# Cluster Autoscaler Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
    component: cluster-autoscaler
spec:
  replicas: 2  # High availability
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8085"
        prometheus.io/path: "/metrics"
        cluster-autoscaler.kubernetes.io/safe-to-evict: "false"
    spec:
      priorityClassName: system-cluster-critical
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cluster-autoscaler
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      - effect: NoSchedule
        key: node-role.kubernetes.io/control-plane
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values: ["cluster-autoscaler"]
              topologyKey: kubernetes.io/hostname
      nodeSelector:
        kubernetes.io/os: linux
        kubernetes.io/arch: amd64
        node-role.kubernetes.io/control-plane: ""
      containers:
      - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.2
        name: cluster-autoscaler
        resources:
          limits:
            cpu: 1000m
            memory: 2Gi
          requests:
            cpu: 500m
            memory: 1Gi
        command:
        - ./cluster-autoscaler
        - --v=2
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste,priority
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/bmad-cluster
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
        - --scale-down-enabled=true
        - --scale-down-delay-after-add=2m
        - --scale-down-delay-after-delete=10s
        - --scale-down-delay-after-failure=3m
        - --scale-down-unneeded-time=2m
        - --scale-down-utilization-threshold=0.5
        - --max-node-provision-time=15m
        - --max-graceful-termination-sec=600
        - --max-empty-bulk-delete=20
        - --max-nodes-total=200
        - --cores-total=0-5000
        - --memory-total=0-5000GiB
        - --gpu-total=0-16:0-16
        - --scan-interval=10s
        - --scale-down-candidates-pool-ratio=0.1
        - --scale-down-candidates-pool-min-count=50
        - --scale-down-non-empty-candidates-count=30
        - --max-bulk-soft-taint-count=10
        - --max-bulk-soft-taint-time=3s
        - --ignore-taint=node.kubernetes.io/not-ready
        - --ignore-taint=node.kubernetes.io/unreachable
        - --ignore-taint=node.kubernetes.io/pid-pressure
        - --ignore-taint=node.kubernetes.io/memory-pressure
        - --ignore-taint=node.kubernetes.io/disk-pressure
        - --ignore-taint=node.kubernetes.io/network-unavailable
        - --write-status-configmap=true
        - --max-inactivity=10m
        - --max-failing-time=15m
        - --ok-total-unready-count=3
        - --max-total-unready-percentage=45
        - --prometheus-address=0.0.0.0:8085
        - --profiling=false
        env:
        - name: AWS_REGION
          value: us-east-1
        - name: AWS_STS_REGIONAL_ENDPOINTS
          value: regional
        ports:
        - name: http
          containerPort: 8085
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health-check
            port: 8085
          initialDelaySeconds: 60
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /health-check
            port: 8085
          initialDelaySeconds: 10
          timeoutSeconds: 5
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - name: ssl-certs
          mountPath: /etc/ssl/certs/ca-certificates.crt
          subPath: ca-certificates.crt
          readOnly: true
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: ssl-certs
        hostPath:
          path: /etc/ssl/certs/ca-certificates.crt
          type: File
      - name: tmp
        emptyDir: {}

---
# Node Pool Configuration for Performance Workloads
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-pool-configurations
  namespace: kube-system
  labels:
    component: cluster-autoscaling
data:
  node-pools.yaml: |
    node_pools:
      # High-performance compute nodes for API workloads
      compute_optimized:
        name: "bmad-compute"
        min_size: 3
        max_size: 50
        desired_capacity: 6
        instance_types: ["c6i.xlarge", "c6i.2xlarge", "c6i.4xlarge"]
        ami_family: "AmazonLinux2"
        volume_size: 100
        volume_type: "gp3"
        volume_iops: 3000
        volume_throughput: 125
        labels:
          node-type: "compute-optimized"
          workload-type: "api"
          instance-category: "performance"
        taints:
        - key: "dedicated"
          value: "compute"
          effect: "NoSchedule"
        tags:
          NodeType: "ComputeOptimized"
          Environment: "Production"
          CostCenter: "DataEngineering"
          AutoScaling: "Enabled"

      # Memory-optimized nodes for analytics and ML
      memory_optimized:
        name: "bmad-memory"
        min_size: 2
        max_size: 30
        desired_capacity: 4
        instance_types: ["r6i.xlarge", "r6i.2xlarge", "r6i.4xlarge"]
        ami_family: "AmazonLinux2"
        volume_size: 200
        volume_type: "gp3"
        volume_iops: 4000
        volume_throughput: 250
        labels:
          node-type: "memory-optimized"
          workload-type: "analytics"
          instance-category: "memory-intensive"
        taints:
        - key: "dedicated"
          value: "analytics"
          effect: "NoSchedule"
        tags:
          NodeType: "MemoryOptimized"
          Environment: "Production"
          CostCenter: "DataScience"

      # General purpose nodes for mixed workloads
      general_purpose:
        name: "bmad-general"
        min_size: 2
        max_size: 40
        desired_capacity: 4
        instance_types: ["m6i.large", "m6i.xlarge", "m6i.2xlarge"]
        ami_family: "AmazonLinux2"
        volume_size: 80
        volume_type: "gp3"
        volume_iops: 3000
        volume_throughput: 125
        labels:
          node-type: "general-purpose"
          workload-type: "mixed"
          instance-category: "balanced"
        tags:
          NodeType: "GeneralPurpose"
          Environment: "Production"
          CostCenter: "Platform"

      # Spot instances for cost optimization
      spot_compute:
        name: "bmad-spot"
        min_size: 0
        max_size: 50
        desired_capacity: 0
        capacity_type: "SPOT"
        instance_types: ["c5.large", "c5.xlarge", "c4.large", "c4.xlarge", "m5.large", "m5.xlarge"]
        ami_family: "AmazonLinux2"
        volume_size: 50
        volume_type: "gp3"
        labels:
          node-type: "spot-instance"
          workload-type: "batch"
          instance-category: "cost-optimized"
        taints:
        - key: "spot"
          value: "true"
          effect: "NoSchedule"
        tags:
          NodeType: "Spot"
          Environment: "Production"
          CostCenter: "CostOptimization"

      # GPU nodes for ML workloads
      gpu_optimized:
        name: "bmad-gpu"
        min_size: 0
        max_size: 10
        desired_capacity: 0
        instance_types: ["g4dn.xlarge", "g4dn.2xlarge", "g4dn.4xlarge"]
        ami_family: "AmazonLinux2"
        volume_size: 200
        volume_type: "gp3"
        volume_iops: 5000
        volume_throughput: 500
        labels:
          node-type: "gpu-optimized"
          workload-type: "ml"
          instance-category: "gpu"
          accelerator: "nvidia-tesla-t4"
        taints:
        - key: "nvidia.com/gpu"
          value: "true"
          effect: "NoSchedule"
        tags:
          NodeType: "GPU"
          Environment: "Production"
          CostCenter: "MachineLearning"

    # Scaling policies per node pool
    scaling_policies:
      compute_optimized:
        scale_up_policy:
          adjustment_type: "ChangeInCapacity"
          scaling_adjustment: 2
          cooldown_period: 180
          metric_threshold: 70  # CPU utilization
        scale_down_policy:
          adjustment_type: "ChangeInCapacity"
          scaling_adjustment: -1
          cooldown_period: 300
          metric_threshold: 30

      memory_optimized:
        scale_up_policy:
          adjustment_type: "ChangeInCapacity"
          scaling_adjustment: 1
          cooldown_period: 300
          metric_threshold: 80  # Memory utilization
        scale_down_policy:
          adjustment_type: "ChangeInCapacity"
          scaling_adjustment: -1
          cooldown_period: 600
          metric_threshold: 40

      general_purpose:
        scale_up_policy:
          adjustment_type: "PercentChangeInCapacity"
          scaling_adjustment: 50
          cooldown_period: 240
          metric_threshold: 75
        scale_down_policy:
          adjustment_type: "PercentChangeInCapacity"
          scaling_adjustment: -25
          cooldown_period: 300
          metric_threshold: 35

---
# Priority Expander Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-autoscaler-priority-expander
  namespace: kube-system
  labels:
    app: cluster-autoscaler
data:
  priorities: |
    10:
      - .*spot.*
    20:
      - .*general.*
    30:
      - .*compute.*
    40:
      - .*memory.*
    50:
      - .*gpu.*

---
# Node Termination Handler DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: aws-node-termination-handler
  namespace: kube-system
  labels:
    app: aws-node-termination-handler
spec:
  selector:
    matchLabels:
      app: aws-node-termination-handler
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
  template:
    metadata:
      labels:
        app: aws-node-termination-handler
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9092"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: aws-node-termination-handler
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
      - name: aws-node-termination-handler
        image: public.ecr.aws/aws-ec2/aws-node-termination-handler:v1.19.0
        imagePullPolicy: IfNotPresent
        securityContext:
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
          runAsGroup: 1000
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 50m
            memory: 64Mi
        env:
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: SPOT_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: DELETE_LOCAL_DATA
          value: "true"
        - name: IGNORE_DAEMON_SETS
          value: "true"
        - name: GRACE_PERIOD
          value: "30"
        - name: POD_TERMINATION_GRACE_PERIOD
          value: "30"
        - name: INSTANCE_METADATA_URL
          value: "http://169.254.169.254"
        - name: NODE_TERMINATION_GRACE_PERIOD
          value: "120"
        - name: WEBHOOK_URL
          value: ""
        - name: WEBHOOK_HEADERS
          value: ""
        - name: WEBHOOK_TEMPLATE
          value: ""
        - name: DRY_RUN
          value: "false"
        - name: ENABLE_PROMETHEUS_SERVER
          value: "true"
        - name: PROMETHEUS_SERVER_PORT
          value: "9092"
        - name: METADATA_TRIES
          value: "3"
        - name: CORDON_ONLY
          value: "false"
        - name: TAINT_NODE
          value: "false"
        - name: JSON_LOGGING
          value: "false"
        - name: LOG_LEVEL
          value: "info"
        - name: ENABLE_SQS_TERMINATION_DRAINING
          value: "false"
        ports:
        - name: http-metrics
          protocol: TCP
          containerPort: 9092
        livenessProbe:
          httpGet:
            path: /healthz
            port: 9092
          initialDelaySeconds: 5
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: 9092
          initialDelaySeconds: 5
          periodSeconds: 5
        volumeMounts:
        - name: uptime
          mountPath: /proc/uptime
          readOnly: true
        - name: var-run
          mountPath: /var/run/dbus/system_bus_socket
          readOnly: true
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: uptime
        hostPath:
          path: /proc/uptime
      - name: var-run
        hostPath:
          path: /var/run/dbus/system_bus_socket
      - name: tmp
        emptyDir: {}
      tolerations:
      - operator: Exists
        effect: NoSchedule
      - operator: Exists
        effect: NoExecute

---
# AWS Node Termination Handler ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aws-node-termination-handler
  namespace: kube-system
  labels:
    app: aws-node-termination-handler
  annotations:
    eks.amazonaws.com/role-arn: "arn:aws:iam::123456789012:role/aws-node-termination-handler-role"

---
# AWS Node Termination Handler RBAC
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: aws-node-termination-handler
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "patch", "update"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "get", "delete"]
- apiGroups: [""]
  resources: ["pods/eviction"]
  verbs: ["create"]
- apiGroups: ["extensions", "apps"]
  resources: ["daemonsets"]
  verbs: ["get"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: aws-node-termination-handler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: aws-node-termination-handler
subjects:
- kind: ServiceAccount
  name: aws-node-termination-handler
  namespace: kube-system

---
# Node Resource Monitor DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-resource-monitor
  namespace: kube-system
  labels:
    app: node-resource-monitor
    component: monitoring
spec:
  selector:
    matchLabels:
      app: node-resource-monitor
  template:
    metadata:
      labels:
        app: node-resource-monitor
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9100"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: node-resource-monitor
      securityContext:
        runAsNonRoot: false  # Need root for node metrics
        runAsUser: 0
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      hostNetwork: true
      hostPID: true
      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.6.1
        imagePullPolicy: IfNotPresent
        args:
        - --path.rootfs=/host
        - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)
        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$$
        - --collector.netdev.device-exclude=^(cali.+|tunl.+|dummy.+|kube.+|flannel.+|cni.+|docker.+|veth.+|.*-ns.+|lo.*)$$
        - --collector.netstat
        - --collector.vmstat
        - --collector.processes
        - --collector.systemd
        - --collector.tcpstat
        - --collector.meminfo_numa
        - --web.listen-address=0.0.0.0:9100
        - --web.telemetry-path=/metrics
        - --log.level=info
        ports:
        - name: metrics
          containerPort: 9100
          protocol: TCP
        resources:
          limits:
            cpu: 200m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        securityContext:
          readOnlyRootFilesystem: true
          runAsNonRoot: false
          runAsUser: 0
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: root
          mountPath: /host
          readOnly: true
        - name: tmp
          mountPath: /tmp

      - name: cadvisor
        image: gcr.io/cadvisor/cadvisor:v0.47.2
        imagePullPolicy: IfNotPresent
        args:
        - --housekeeping_interval=10s
        - --max_housekeeping_interval=15s
        - --event_storage_event_limit=default=0
        - --event_storage_age_limit=default=0
        - --store_container_labels=false
        - --whitelisted_container_labels=io.kubernetes.container.name,io.kubernetes.pod.name,io.kubernetes.pod.namespace
        - --docker_only=false
        - --disable_metrics=percpu,process,sched,tcp,udp,disk,diskIO,accelerator,hugetlb,referenced_memory,cpu_topology,resctrl
        - --enable_load_reader=true
        - --port=8080
        ports:
        - name: cadvisor
          containerPort: 8080
          protocol: TCP
        resources:
          limits:
            cpu: 300m
            memory: 500Mi
          requests:
            cpu: 150m
            memory: 200Mi
        securityContext:
          readOnlyRootFilesystem: true
          runAsNonRoot: false
          runAsUser: 0
          privileged: false
          capabilities:
            add:
            - SYS_ADMIN
            drop:
            - ALL
        volumeMounts:
        - name: rootfs
          mountPath: /rootfs
          readOnly: true
        - name: var-run
          mountPath: /var/run
          readOnly: true
        - name: sys-fs-cgroup
          mountPath: /sys/fs/cgroup
          readOnly: true
        - name: var-lib-docker
          mountPath: /var/lib/docker
          readOnly: true
        - name: dev-disk
          mountPath: /dev/disk
          readOnly: true
        - name: tmp
          mountPath: /tmp

      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /
      - name: rootfs
        hostPath:
          path: /
      - name: var-run
        hostPath:
          path: /var/run
      - name: sys-fs-cgroup
        hostPath:
          path: /sys/fs/cgroup
      - name: var-lib-docker
        hostPath:
          path: /var/lib/docker
      - name: dev-disk
        hostPath:
          path: /dev/disk
      - name: tmp
        emptyDir: {}
      tolerations:
      - operator: Exists
        effect: NoSchedule
      - operator: Exists
        effect: NoExecute

---
# Node Resource Monitor ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: node-resource-monitor
  namespace: kube-system

---
# Node Resource Monitor Service
apiVersion: v1
kind: Service
metadata:
  name: node-resource-monitor
  namespace: kube-system
  labels:
    app: node-resource-monitor
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9100"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: node-exporter
    port: 9100
    targetPort: 9100
    protocol: TCP
  - name: cadvisor
    port: 8080
    targetPort: 8080
    protocol: TCP
  selector:
    app: node-resource-monitor

---
# Cluster Autoscaler Service for Metrics
apiVersion: v1
kind: Service
metadata:
  name: cluster-autoscaler-metrics
  namespace: kube-system
  labels:
    app: cluster-autoscaler
    component: metrics
spec:
  type: ClusterIP
  ports:
  - name: metrics
    port: 8085
    targetPort: 8085
    protocol: TCP
  selector:
    app: cluster-autoscaler

---
# ServiceMonitor for Cluster Autoscaler
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
    component: monitoring
spec:
  selector:
    matchLabels:
      app: cluster-autoscaler
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    honorLabels: true

---
# ServiceMonitor for Node Resource Monitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: node-resource-monitor
  namespace: kube-system
  labels:
    app: node-resource-monitor
    component: monitoring
spec:
  selector:
    matchLabels:
      app: node-resource-monitor
  endpoints:
  - port: node-exporter
    interval: 15s
    path: /metrics
    honorLabels: true
  - port: cadvisor
    interval: 15s
    path: /metrics
    honorLabels: true

---
# PrometheusRule for Cluster Autoscaling Alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: cluster-autoscaling-alerts
  namespace: kube-system
  labels:
    app: cluster-autoscaler
    component: alerting
spec:
  groups:
  - name: cluster-autoscaling.rules
    interval: 30s
    rules:
    # Cluster Autoscaler is not running
    - alert: ClusterAutoscalerDown
      expr: absent(up{job="cluster-autoscaler"}) == 1
      for: 5m
      labels:
        severity: critical
        component: cluster-autoscaler
      annotations:
        summary: "Cluster Autoscaler is down"
        description: "Cluster Autoscaler has been down for more than 5 minutes"

    # Node scaling issues
    - alert: NodesNotScaling
      expr: cluster_autoscaler_nodes_count{type="ready"} == cluster_autoscaler_nodes_count{type="ready"} offset 30m
      for: 30m
      labels:
        severity: warning
        component: cluster-autoscaler
      annotations:
        summary: "Cluster nodes not scaling"
        description: "Node count has not changed in 30 minutes despite potential scaling needs"

    # High node resource utilization
    - alert: HighNodeCPUUtilization
      expr: |
        (
          (1 - (avg_over_time(node_cpu_seconds_total{mode="idle"}[5m]) / avg_over_time(node_cpu_seconds_total[5m]))) * 100
        ) > 85
      for: 10m
      labels:
        severity: warning
        component: node-resource-monitor
      annotations:
        summary: "High node CPU utilization"
        description: "Node {{ $labels.instance }} CPU utilization is {{ $value }}%"

    # High node memory utilization
    - alert: HighNodeMemoryUtilization
      expr: |
        (
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100
        ) > 85
      for: 10m
      labels:
        severity: warning
        component: node-resource-monitor
      annotations:
        summary: "High node memory utilization"
        description: "Node {{ $labels.instance }} memory utilization is {{ $value }}%"

    # Node not ready
    - alert: NodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
        component: node-monitor
      annotations:
        summary: "Node not ready"
        description: "Node {{ $labels.node }} has been not ready for more than 5 minutes"

    # Spot instance interruption
    - alert: SpotInstanceTermination
      expr: aws_node_termination_handler_actions_total{action="cordon"} > 0
      for: 0m
      labels:
        severity: warning
        component: aws-node-termination-handler
      annotations:
        summary: "Spot instance termination detected"
        description: "Spot instance on node {{ $labels.node }} is being terminated"

    # Cluster resource pressure
    - alert: ClusterResourcePressure
      expr: |
        (
          sum(kube_pod_container_resource_requests{resource="cpu"}) /
          sum(kube_node_status_allocatable{resource="cpu"})
        ) > 0.8
      for: 10m
      labels:
        severity: warning
        component: cluster-resource-monitor
      annotations:
        summary: "Cluster CPU resource pressure"
        description: "Cluster CPU resource utilization is {{ $value | humanizePercentage }}"